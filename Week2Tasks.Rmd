---
title: "Week 2: Tasks 2 and 3"
author: "Chia Ying Lee"
date: "August 6, 2017"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

library(magrittr)
library(data.table)
library(knitr)
library(ggplot2)
library(Matrix)
library(quanteda)

setwd("C:/Users/lchia/Coursera/DataScienceSpecialization/Course10")
```

The [`quanteda`](https://cran.r-project.org/web/packages/quanteda/vignettes/quickstart.html) package has a lot of (R-friendly) functionality for NLP.


```{r load-corpora, cache = T}
path.corpora <- "./Coursera-SwiftKey/final/en_US/"

files.corpora <- c(blogs = "en_US.blogs.txt",
                   news = "en_US.news.txt",
                   twitter = "en_US.twitter.txt")

# corpora_raw <- lapply(files.corpora, function(f) 
#     readLines(paste0(path.corpora, f), encoding = "UTF-8", skipNul = T))

load("MilestoneReport_cache/corpora_raw.Rdata", verbose = T)
```


```{r get-subset-corpus, cache = T}
pct.subset <- .1

set.seed(1)
corpora_raw_subset <- lapply(corpora_raw, function(cps)
    cps[rbinom(length(cps), 1, pct.subset) == 1])

corpora <- lapply(corpora_raw_subset, corpus)

save(corpora, file = "data/corpora.Rdata")
```

```{r}
sent_corpora <- lapply(corpora_raw_subset, function(cps_raw) {
    sent <- tokenize(cps_raw, what = "sentence")
    docvars <- data.table(DocumentSID = rep(seq_along(sent), 
                                            times = sapply(sent, length)))
    # Each sentence is tagged with the document it originated from    
    corpus(unlist(sent), docvars = docvars)
})

save(sent_corpora, file = "data/sent_copora.Rdata")
```


```{r}
sent_tokens <- lapply(sent_corpora, function(cps) tokens(cps, what = "word", 
                                                         remove_twitter = T,
                                                         remove_numbers = T,
                                                         remove_punct = T,
                                                         remove_symbols = T))

sent_tokens_noStopwords <- lapply(sent_tokens, 
                                  function(tkn) tokens_remove(tkn, stopwords()))
beep(10)
```


## Document Term Frequency

- Keep stopwords, punctuation
- Detect end of sentence (no need for word suggestions) and beginning of sentence.
- Convert numbers into a single token class
- Keep case-sensitive?


```{r getDFM_Ngrams--function}
#' Helper function to 
#' @param InputCorpus Supply either a corpus or tokens. Cannot both be NULL.
#' @param InputTokens Supply either a corpus or tokens. Cannot both be NULL.
#' @param N Arguments for tokens_ngrams function
#' @param skip Arguments for tokens_ngrams function
#' @param remove A character vector of user-supplied features to ignore, such as
#'               "stop words", passed to the dfm function
#' @param ... Arguments to be passed to the tokens function. This could include
#'            remove_numbers, remove_punct, remove_symbols, remove_twitter, etc.
getDFM_Ngrams <- function(InputCorpus = NULL, N = 1, skip = 0L, 
                          InputTokens = NULL, remove = NULL, ...) {
    if (is.null(InputTokens)) InputTokens <- tokens(InputCorpus, ...) 
    if (!is.null(remove)) InputTokens <- tokens_remove(InputTokens, remove) 
    
    tokens_ngrams(InputTokens, N, skip) %>% 
        dfm(verbose = FALSE, ...)
}
```


```{r generate-Ngrams}
# 1grams
DFM_1grams <- lapply(sent_tokens, function(tkn) 
    getDFM_Ngrams(InputTokens = tkn, N = 1))

DFM_noStopwords_1grams <- lapply(sent_tokens_noStopwords, function(tkn) 
    getDFM_Ngrams(InputTokens = tkn, N = 1))

# 2grams
DFM_2grams <- lapply(sent_tokens, function(tkn) 
    getDFM_Ngrams(InputTokens = tkn, N = 2))

DFM_noStopwords_2grams <- lapply(sent_tokens_noStopwords, function(tkn) 
    getDFM_Ngrams(InputTokens = tkn, N = 2))


# 3grams
DFM_3grams <- lapply(sent_tokens, function(tkn) 
    getDFM_Ngrams(InputTokens = tkn, N = 3))

DFM_noStopwords_3grams <- lapply(sent_tokens_noStopwords, function(tkn) 
    getDFM_Ngrams(InputTokens = tkn, N = 3))



# 4grams
DFM_4grams <- lapply(sent_tokens, function(tkn) 
    getDFM_Ngrams(InputTokens = tkn, N = 4))

DFM_noStopwords_4grams <- lapply(sent_tokens_noStopwords, function(tkn) 
    getDFM_Ngrams(InputTokens = tkn, N = 4))


# Also try 2grams that skip over 1 word.
twitter.sent_dfm2grams1 <- getDFM_Ngrams(twitter.sent_corpus, 2, 1, remove_punct = T)
```

View some of the DFMs.

```{r view-dfm}
# Show a few rows/columns of the matrix
twitter.sent_dfm1grams[1:10, 1:7]

# Show the first few feature names
featnames(twitter.sent_dfm2grams1) %>% head

# Show the most frequent features
topfeatures(twitter.sent_dfm2grams, 20)

# Plot the word cloud
set.seed(42)
textplot_wordcloud(twitter.sent_dfm3grams, 
                   min.freq = 3, 
                   random.order = FALSE,
                   rot.per = .25, 
                   colors = RColorBrewer::brewer.pal(8, "Dark2"))
title(main = "Word Cloud for Twitter 3-grams")
```




```{r}
load("MilestoneReport_cache/dfm.Rdata", verbose = T)
```



> Q: How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?

A lot of the most frequently used words are stopwords, so to get a meaningful answer to this question, we use stemming and remove stopwords.

```{r analysis-word-instances, cache = T, warning=F}
twitter.sent.NoStopwords_dfm <- getDFM_Ngrams(twitter.sent_corpus, 1, remove_punct = T, stem = T, remove = stopwords(kind = "english"))

wordFreq <- colSums(twitter.sent.NoStopwords_dfm) %>% .[order(., decreasing = T)]

head(wordFreq)

mean(cumsum(wordFreq/sum(wordFreq)) <= .5)
mean(cumsum(wordFreq/sum(wordFreq)) <= .9)
```

Later on in the project, knowing which words make up the majority of word instances can allow us to focus the prediction on just these words, thereby reducing the size of the prediction model data.

## Predictive Model

Using the 1-, 2-, 3-, and 4-gram frequencies, we'll build a predictive model to predict the next couple of words given a partially typed sentence.

### The trivial case: no words typed
Let's start with the simplest model: if no words have been typed, our prediction will be the most probable 1-gram.

```{r Predict1Word-Using0Word}
#' Predict one word if no words have been typed
Predict1Word.Using0Word <- function() {
    topfeatures(twitter.sent_dfm1grams, n = 1) %>% names
}

Predict1Word.Using0Word()
```


### Using the last 1, 2, 3 words to predict the next word

Given the last word of a partial sentence, we want to predict the next word. Assuming the last word is in our dictionary (we'll handle the contrary case later), we'll build a Markov chain model using the 2-gram frequencies, as follows.

Let $\mathcal{W} = \{W_1, \dots, W_M\}$ be the dictionary, and let $n_{ij}$ be the frequency of the bigram $(W_i, W_j)$. If the last word is $W_i$, and assuming that there will be a next word (i.e., it's not the end of the sentence), the probability of the next word is

$$ P_{ij} = \frac{n_{ij}}{\sum_j n_{ij}} .$$

Then, the prediction for the next word is the most probable $W_j$:

$$ j = \argmax_{j'} \{P_{ij'}\}. $$

```{r}
Predict1Word.UsingNgram__Generator__ <- function(DFMs) {
    Nmax <- length(DFMs)
    
    freqNgrams <- lapply(1:Nmax, function(N) {
        DFM <- DFMs[[N]]
        freq <- colSums(DFM) %>% 
            as.data.table(keep.rownames = "Ngram") %>%
            .[, `:=`(paste0("Word", N:1), 
                     lapply(strsplit(Ngram, "_"), function(s) as.list(s[1:N])) %>% 
                         rbindlist)
              ] %>%
            .[, Ngram := NULL] %>%
            setnames(".", "count")
        
        for (n in 1:N) freq <- freq[get(paste0("Word", n)) != ""]
        
        setkeyv(freq, cols = paste0("Word", 1:N))
    })
    
    freqNgrams.top <- lapply(freqNgrams, function(freq) {
        word.cols <- setdiff(colnames(freq), "count")
        freq[, .SD[which.max(count)], by = word.cols]
    })

    function(word1, nextWords = NULL) {
        if (is.null(nextWords)) {
            freq2gram.top[.(char_tolower(word1)), Word2]
        } else {
            freq2gram[.(char_tolower(word1), 
                        char_tolower(word2s))][which.max(`.`), Word2]
        }
    }
}
```


```{r Predict1Word-Using1Word}
Predict1Word.Using1Word__Generator__ <- function(DFM) {
    freq2gram <- colSums(DFM) %>% 
        as.data.table(keep.rownames = "Word1") %>%
        .[, `:=`(c("Word1", "Word2"), 
                 lapply(strsplit(Word1, "_"), function(s) as.list(s[1:2])) %>% rbindlist)] %>%
        .[Word1 != "" & Word2 != ""] %>% # Remove empty words 
        setkey(Word1, Word2)
    
    freq2gram.top <- freq2gram[, .SD[which.max(`.`)], by = Word1] %>%
        setkey(Word1)
    
    function(word1, word2s = NULL) {
        if (is.null(word2s)) {
            freq2gram.top[.(char_tolower(word1)), Word2]
        } else {
            freq2gram[.(char_tolower(word1), 
                        char_tolower(word2s))][which.max(`.`), Word2]
        }
    }
}

Predict1Word.Using1Word <- Predict1Word.Using1Word__Generator__(DFM)
```

Now let's try predicting the next word.
```{r Examples--Predict1Word-Using1Word}
Predict1Word.Using1Word("the")
Predict1Word.Using1Word("go")
Predict1Word.Using1Word("yayy") # The word 'yayy' is in the corpus but is not the first word of a 2-gram
Predict1Word.Using1Word("unknown") # The word 'unknown' is not in the corpus
```



```{r Predict1Word-Using2Word}
Predict1Word.Using2Word__Generator__ <- function(DFM) {
    freq3gram <- colSums(DFM) %>%
        as.data.table(keep.rownames = "Word1") %>%
        .[, `:=`(c(paste0("Word", 1:3)),
                 lapply(strsplit(Word1, "_"), function(s) as.list(s[1:3])) %>%
                     rbindlist)
          ] %>%
        .[Word1 != "" & Word2 != "" & Word3 != ""] %>%
        setkey(Word1, Word2, Word3)
    
    freq3gram.top <- freq3gram[, .SD[which.max(`.`)], by = .(Word1, Word2)] %>%
        setkey(Word1, Word2)
    
    function(word1, word2, word3s = NULL) {
        if (is.null(word3s)) {
            freq3gram.top[.(char_tolower(word1), char_tolower(word2)), Word3]
        } else {
            freq3gram[.(char_tolower(word1), 
                        char_tolower(word2), 
                        char_tolower(word3s))][which.max(`.`), Word3]
        }
    }
}

Predict1Word.Using2Word <- Predict1Word.Using2Word__Generator__(dfm.3grams$blogs)
```



```{r Examples--Predict1Word-Using2Word}
Predict1Word.Using2Word("thank", "you")
```



```{r Predict1Word-Using3Word}
Predict1Word.Using3Word__Generator__ <- function(DFM) {
    freq4gram <- colSums(DFM) %>%
        as.data.table(keep.rownames = "Word1") %>%
        .[, `:=`(c(paste0("Word", 1:4)),
                 lapply(strsplit(Word1, "_"), function(s) as.list(s[1:4])) %>%
                     rbindlist)
          ] %>%
        .[Word1 != "" & Word2 != "" & Word3 != "" & Word4 != ""] %>%
        setkey(Word1, Word2, Word3, Word4)
    
    freq4gram.top <- freq4gram[, .SD[which.max(`.`)], by = .(Word1, Word2, Word3)] %>%
        setkey(Word1, Word2, Word3)
    
    function(word1, word2, word3, word4s = NULL) {
        if (is.null(word4s)) {
            freq4gram.top[.(char_tolower(word1), 
                            char_tolower(word2),
                            char_tolower(word3)), Word4]
        } else {
            freq4gram[.(char_tolower(word1), 
                        char_tolower(word2),
                        char_tolower(word3),
                        char_tolower(word4s))][which.max(`.`), Word4]
        }
    }
}

Predict1Word.Using3Word <- Predict1Word.Using3Word__Generator__(dfm.4grams$blogs)
```



<!-- # Outtakes -->

<!-- [`tokenizers`](https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html) -->

<!-- ```{r} -->
<!-- # library(tokenizers) -->
<!-- #  -->
<!-- # word_tokens <- tokenize_words(lapply(twitter, as.character)) -->
<!-- # nWords <- sapply(word_tokens, length) -->
<!-- # bigrams <- tokenize_ngrams(twitter, n = 2, n_min = 2) -->

<!-- ``` -->



<!-- ```{r} -->
<!-- # library(tm) -->
<!-- #  -->
<!-- # twitter.VCorpus <- VCorpus(VectorSource(twitter)) -->
<!-- #  -->
<!-- # inspect(twitter.VCorpus[1:3]) -->
<!-- #  -->
<!-- # dtm <- DocumentTermMatrix(twitter.VCorpus[1:10]) -->

<!-- ``` -->


<!-- ```{r} -->
<!-- # Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_144') -->
<!-- # library(rJava) -->
<!-- # library(openNLP) -->
<!-- # library(NLP) -->
<!-- #  -->
<!-- # twitter.String <- as.String(twitter) -->
<!-- # Sent_Token_Annotator <- Maxent_Sent_Token_Annotator() -->
<!-- # Sent_Tokens <- annotate(twitter.String, Sent_Token_Annotator) -->
<!-- #  -->
<!-- # Word_Token_Annotator <- Maxent_Word_Token_Annotator() -->
<!-- # Word_Tokens <- annotate(twitter.String, f = Word_Token_Annotator, a = Sent_Tokens) -->
<!-- ``` -->

